# dagster-slurm

[![image](https://img.shields.io/pypi/v/dagster-slurm.svg)](https://pypi.python.org/pypi/dagster-slurm)
[![image](https://img.shields.io/pypi/l/dagster-slurm.svg)](https://pypi.python.org/pypi/dagster-slurm)
[![image](https://img.shields.io/pypi/pyversions/dagster-slurm.svg)](https://pypi.python.org/pypi/dagster-slurm)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)

---

Integrating dagster to orchestrate slurm jobs for distributed systems like ray for a better developer experience on supercomputers.

As part of the hackathon (https://www.openhackathons.org/s/siteevent/a0CUP000013Tp8f2AC/se000375) we intend to work on this integration.
We are looking for more hands to join in - or review the task list so that we can make sure we are not missing anything.

- Tasks for implementation: https://github.com/orgs/ascii-supply-networks/projects/4
- Project lives here https://github.com/ascii-supply-networks/dagster-slurm

See the (draft) [documentation](https://ascii-supply-networks.github.io/dagster-slurm/)

> We are actively looking for contributions to bring this package to life together


## developing

```bash
docker compose up --build -d
ssh submitter@localhost -p 2222
# password: submitter
sinfo
```

## basic distribution

initial setup

```bash
curl -fsSL https://pixi.sh/install.sh | sh
bash
pixi global install pixi-unpack
```

environment setup

```bash
cd examples
pixi run pack
# if using an ARM host
# pixi run pack-aarch

scp -P 2222 environment.tar submitter@localhost:/home/submitter
ssh submitter@localhost -p 2222

# tar -xvf environment.tar
pixi exec pixi-unpack environment.tar
source ./activate.sh
```

as a result:

```bash
source /home/submitter/activate.sh
```

is now available on all the cluster nodes

## ray

docs: https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm.html#id7

- get `slurm-template.sh` but modified https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm-template.html#slurm-template

```bash
#!/bin/bash
# shellcheck disable=SC2206
# THIS FILE IS GENERATED BY AUTOMATION SCRIPT! PLEASE REFER TO ORIGINAL SCRIPT!
# THIS FILE IS A TEMPLATE AND IT SHOULD NOT BE DEPLOYED TO PRODUCTION!
${PARTITION_OPTION}
#SBATCH --job-name=${JOB_NAME}
#SBATCH --output=${JOB_NAME}.log
${GIVEN_NODE}
### This script works for any number of nodes, Ray will find and manage all resources
#SBATCH --nodes=${NUM_NODES}
#SBATCH --exclusive
#SBATCH --signal=TERM@60
### Give all resources to a single Ray task, ray can manage the resources internally
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-task=${NUM_GPUS_PER_NODE}

set -euxo pipefail

# Shared sentinel file on a shared filesystem (e.g., your home/project dir)
SENTINEL="${SLURM_SUBMIT_DIR:-$PWD}/.ray_shutdown.${SLURM_JOB_ID:-$$}"

cleanup() {
  echo "--- Performing cleanup ---"
  : > "$SENTINEL"      # signal workers/head to stop

  # Optional but recommended on shared FS: let workers notice the sentinel.
  sleep 1

  wait || true   # wait for the srun steps to exit cleanly
  rm -f "$SENTINEL" || true    # tidy up
  echo "--- cleanup complete ---"
}
trap cleanup EXIT SIGINT SIGTERM

# ===== Environment (module/conda/etc) =====
${LOAD_ENV}

# ===== Compute Ray head address =====
redis_password=$(uuidgen); export redis_password

nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
nodes_array=($nodes)

head_node=${nodes_array[0]}
ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)

# Normalize to IPv4 if multiple IPs
if [[ "$ip" == *" "* ]]; then
  IFS=' ' read -ra ADDR <<< "$ip"
  if [[ ${#ADDR[0]} -gt 16 ]]; then ip=${ADDR[1]}; else ip=${ADDR[0]}; fi
  echo "Multiple IPs detected; using IPv4 $ip"
fi

port=6379
ip_head="$ip:$port"
export ip_head
export RAY_ADDRESS="$ip_head"
echo "Head node: $head_node | IP Head: $ip_head"

# ===== Start HEAD (daemon step that watches SENTINEL) =====
echo "STARTING HEAD at ${head_node}"
srun --export=ALL --nodes=1 --ntasks=1 -w "$head_node" \
  bash --noprofile --norc -lc '
    set -euo pipefail
    SENTINEL="'"$SENTINEL"'"
    # On TERM/INT from Slurm, stop Ray gracefully then exit 0.
    cleanup_node() { ray stop -v --grace-period 60 || true; exit 0; }
    trap cleanup_node TERM INT

    # Start Ray head and watch for shutdown
    ray start --head -v --node-ip-address="'"$ip"'" --port="'"$port"'" \
      --redis-password="'"$redis_password"'" --block &
    ray_pid=$!

    while :; do
      # If sentinel exists, stop Ray and exit 0
      [[ -f "$SENTINEL" ]] && cleanup_node
      # If Ray died for any reason, just exit 0 (step completes cleanly)
      if ! kill -0 "$ray_pid" 2>/dev/null; then exit 0; fi
      sleep 1
    done
  ' &

# ===== Wait for head ready =====
echo "Waiting for Ray head to be ready..."
for i in {1..40}; do
  if ray status --address "$ip_head" &>/dev/null; then
    echo "Ray head is ready."
    break
  fi
  if [[ $i -eq 40 ]]; then
    echo "Ray head failed to become ready in time." >&2
    exit 1
  fi
  sleep 2
done

# ===== Start WORKERS (each watches the same SENTINEL) =====
worker_num=$((SLURM_JOB_NUM_NODES - 1))
if (( worker_num > 0 )); then
  for ((i = 1; i <= worker_num; i++)); do
    node_i=${nodes_array[$i]}
    echo "STARTING WORKER $i at $node_i"
    srun --export=ALL --nodes=1 --ntasks=1 -w "$node_i" \
      bash --noprofile --norc -lc '
        set -euo pipefail
        SENTINEL="'"$SENTINEL"'"
        cleanup_node() { ray stop -v --grace-period 60 || true; exit 0; }
        trap cleanup_node TERM INT

        ray start -v --address="'"$ip_head"'" --redis-password="'"$redis_password"'" --block &
        ray_pid=$!

        while :; do
          [[ -f "$SENTINEL" ]] && cleanup_node
          if ! kill -0 "$ray_pid" 2>/dev/null; then exit 0; fi
          sleep 1
        done
      ' &
    # small gap avoids thundering herd on startup
    sleep 1
  done
fi

echo "All Ray nodes started. Giving workers a moment to register..."
sleep 5

# ===== Run your workload =====
echo "Running user's command: ${COMMAND_PLACEHOLDER}"
${COMMAND_PLACEHOLDER}
exit_code=$?

echo "User command finished with exit code $exit_code."
echo "Shutting down Ray cluster (creating sentinel)..."
# (trap will create the sentinel and wait for steps)
exit $exit_code
```

- get `slurm-launch.py` inspired by (but modified) https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm-launch.html#slurm-launch

```python
# slurm-launch.py
# Usage with dry run:
# python slurm-launch.py --exp-name test \
#     --activation-script /data/activate.sh \
#     --command "python my_script.py" --dry-run

import argparse
import subprocess
import sys
import time
from pathlib import Path

# Assumes slurm-template.sh is in the same directory as this script
template_file = Path(__file__).parent / "slurm-template.sh"

# Placeholders in the template file
JOB_NAME = "${JOB_NAME}"
NUM_NODES = "${NUM_NODES}"
NUM_GPUS_PER_NODE = "${NUM_GPUS_PER_NODE}"
PARTITION_OPTION = "${PARTITION_OPTION}"
COMMAND_PLACEHOLDER = "${COMMAND_PLACEHOLDER}"
GIVEN_NODE = "${GIVEN_NODE}"
LOAD_ENV = "${LOAD_ENV}"


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--exp-name",
        type=str,
        required=True,
        help="The job name and path to logging file (exp_name.log).",
    )
    parser.add_argument(
        "--num-nodes", "-n", type=int, default=1, help="Number of nodes to use."
    )
    parser.add_argument(
        "--node",
        "-w",
        type=str,
        help="The specified nodes to use. Same format as the "
        "return of 'sinfo'. Default: ''.",
    )
    parser.add_argument(
        "--num-gpus",
        type=int,
        default=0,
        help="Number of GPUs to use in each node. (Default: 0)",
    )
    parser.add_argument(
        "--partition",
        "-p",
        type=str,
    )
    parser.add_argument(
        "--activation-script",
        type=str,
        help="The full path to the shell script to source for activating the environment (e.g., /data/activate.sh).",
        default="",
    )
    parser.add_argument(
        "--command",
        type=str,
        required=True,
        help="The command you wish to execute. For example: "
        " --command 'python test.py'. "
        "Note that the command must be a string.",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="If set, generate the submission script but do not submit it."
    )

    args = parser.parse_args()

    if args.node:
        node_info = "#SBATCH -w {}".format(args.node)
    else:
        node_info = ""

    job_name = "{}_{}".format(
        args.exp_name, time.strftime("%m%d-%H%M", time.localtime())
    )
    
    partition_option = (
        "#SBATCH --partition={}".format(args.partition) if args.partition else ""
    )

    if args.activation_script:
        load_env_command = f"source {args.activation_script}"
    else:
        load_env_command = "# No activation script provided."

    if not template_file.is_file():
        print(f"Error: Template file not found at {template_file}")
        sys.exit(1)
        
    with open(template_file, "r") as f:
        text = f.read()

    text = text.replace(JOB_NAME, job_name)
    text = text.replace(NUM_NODES, str(args.num_nodes))
    text = text.replace(NUM_GPUS_PER_NODE, str(args.num_gpus))
    text = text.replace(PARTITION_OPTION, partition_option)
    text = text.replace(COMMAND_PLACEHOLDER, str(args.command))
    text = text.replace(LOAD_ENV, load_env_command)
    text = text.replace(GIVEN_NODE, node_info)
    text = text.replace(
        "# THIS FILE IS A TEMPLATE AND IT SHOULD NOT BE DEPLOYED TO PRODUCTION!",
        "# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE "
        "RUNNABLE!",
    )

    script_file = "{}.sh".format(job_name)
    with open(script_file, "w") as f:
        f.write(text)

    if args.dry_run:
        print("--- DRY RUN MODE ---")
        print(f"Job script '{script_file}' has been generated but NOT submitted.")
        print("You can inspect the script and submit it manually with:")
        print(f"  sbatch {script_file}")
    else:
        print("Starting to submit job!")
        result = subprocess.run(
            ["sbatch", script_file], 
            capture_output=True, 
            text=True, 
            check=True
        )
        print(result.stdout.strip()) 
        print(
            "Job submitted! Script file is at: <{}>. Log file is at: <{}>".format(
                script_file, "{}.log".format(job_name)
            )
        )
        
    sys.exit(0)
```
- make a mini python file `my_mini_job.py` with contents of:

```python
import ray
ray.init()

@ray.remote
def f(x):
    return x * x

futures = [f.remote(i) for i in range(4)]
print(ray.get(futures)) # [0, 1, 4, 9]
```

```bash
# test run locally (single node)
python my_mini_job.py

# submit to slurm (dry run)
python slurm-launch.py --exp-name test --command "python my_mini_job.py" --num-nodes 2 --activation-script /home/submitter/activate.sh --dry-run
# --- DRY RUN MODE ---
# Job script 'sbatch test_0823-0707.sh' has been generated but NOT submitted.
# You can inspect the script and submit it manually with:
#  sbatch sbatch test_0823-0707.sh

cat test_0823-0707.sh


# now really submit
python slurm-launch.py --exp-name test --command "python my_mini_job.py" --num-nodes 2 --activation-script /home/submitter/activate.sh
```

### slurm monitoring

```bash
# ongoing
squeue -u submitter

# status after completion by job id
sacct -j 1.1
# by user
sacct -u submitter

# all jobs
sacct

# logs (only whilst running, not after completion)
scontrol show job 3
cat $(scontrol show job 1 | grep -oP 'StdOut=\K\S+')
```

debugging

```bash
yum install procps

ps aux | grep ray

# cancel a stuck job
scancel 6
```